{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "机器学习算法之随机森林（Random Forest）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging\n",
    "Bagging方法是ensemble methods中获得用于训练base estimator的数据的重要一环。 正如其名，Bagging方法就是将所有training data放进一个黑色的bag中，黑色意味着我们看不到里面的数据的详细情况，只知道里面有我们的数据集。然后从这个bag中随机抽一部分数据出来用于训练一个base estimator。抽到的数据用完之后我们有两种选择，放回或不放回。\n",
    "\n",
    "既然样本本身可以bagging，那么feature是不是也可以bagging呢？当然可以！bagging完数据本身之后我们可以再bagging features，即从所有特征维度里面随机选取部分特征用于训练。在后面我们会看到，这两个‘随机’就是随机森林的精髓所在。从随机性来看，bagging技术可以有效的减小方差，即减小过拟合程度。\n",
    "\n",
    "在scikit-learn中，我们可以很方便的将bagging技术应用于一个分类器/回归器，提高性能："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree\n",
    "关于决策树，在这里不展开详细探讨，有机会的话另开一篇博客细说。先简单地举一个例子，以下是一棵分类树，决定下班后是否要观看机器学习公开课。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./images/DT-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 随机森林回归\n",
    "\n",
    "算法介绍：\n",
    "\n",
    "       随机森林是决策树的集成算法。随机森林包含多个决策树来降低过拟合的风险。随机森林同样具有易解释性、可处理类别特征、易扩展到多分类问题、不需特征缩放等性质。\n",
    "\n",
    "       随机森林分别训练一系列的决策树，所以训练过程是并行的。因算法中加入随机过程，所以每个决策树又有少量区别。通过合并每个树的预测结果来减少预测的方差，提高在测试集上的性能表现。\n",
    "\n",
    "       随机性体现：\n",
    "1.每次迭代时，对原始数据进行二次抽样来获得不同的训练数据。\n",
    "\n",
    "2.对于每个树节点，考虑不同的随机特征子集来进行分裂。\n",
    "\n",
    "        除此之外，决策时的训练过程和单独决策树训练过程相同。\n",
    "\n",
    "        对新实例进行预测时，随机森林需要整合其各个决策树的预测结果。回归和分类问题的整合的方式略有不同。分类问题采取投票制，每个决策树投票给一个类别，获得最多投票的类别为最终结果。回归问题每个树得到的预测结果为实数，最终的预测结果为各个树预测结果的平均值。\n",
    "\n",
    "        spark.ml支持二分类、多分类以及回归的随机森林算法，适用于连续特征以及类别特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我们可以看到从根节点开始往下会有分支，最终会走向叶子节点，得到分类结果。每一个非叶子节点都是一个特征，上图中共有三维特征。\n",
    "但是决策树的一个劣势就是容易过拟合，下面我们要结合上文提到的bagging技术来中和一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest\n",
    "bagging + decision trees，我们得到了随机森林。将决策树作为base estimator，然后采用bagging技术训练一大堆小决策树，最后将这些小决策树组合起来，这样就得到了一片森林(随机森林)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://backnode.github.io/pages/2015/04/23/random-forest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "召回率和正确率计算\n",
    "对于一个K元的分类结果，我们可以得到一个K∗K的混淆矩阵,得到的举证结果如下图所示。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图所示的结果中不同的元素表示的含义如下： \n",
    "mij ：表示实际分类属于类i，在预测过程中被预测到分类j\n",
    "对于所有的mij可以概括为四种方式不同类型的数据:\n",
    "\n",
    "TP（真正）:真正的分类结果属于i预测的结果也属于i，此时对于 mij 而言i=j\n",
    "FN（假负）:真正的分类结果不属于分类i预测的分类结果也属于分类i\n",
    "TN (真负) :真正的分类结果属于分类i预测的结果不属于分类i\n",
    "FP (假正) :真正的分类结果不属于分类i预测的结果属于分类i \n",
    "那么召回率的计算公式如下： \n",
    "\n",
    "recall=TP/TP+TN\n",
    "\n",
    "precision=TP/TP+FP\n",
    "\n",
    "其中：\n",
    "TPp=mpp:表示对于类p而言预测正确的个体的数目。\n",
    "\n",
    "TNp=∑ki=1,i≠pmpi:表示对本来属于分类p，预测的结果不属于分类p的个数。\n",
    "\n",
    "FPp=∑ki=1,i≠pmip:表示对于本来不属于分类p的个体，预测的结果属于分类p的个数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 随机森林中召回率和正确率的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "// 加载数据\n",
    "val data = MLUtils.loadLibSVMFile(sc, \"data/sample_libsvm_data.txt\")\n",
    "// 将数据随机分配为两份，一份用于训练，一份用于测试\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 随机森林训练参数设置\n",
    "//分类数\n",
    "val numClasses = 2\n",
    "// categoricalFeaturesInfo 为空，意味着所有的特征为连续型变量\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "//树的个数\n",
    "val numTrees = 3 \n",
    "//特征子集采样策略，auto 表示算法自主选取\n",
    "val featureSubsetStrategy = \"auto\" \n",
    "//纯度计算\n",
    "val impurity = \"gini\"\n",
    "//树的最大层次\n",
    "val maxDepth = 4\n",
    "//特征最大装箱数\n",
    "val maxBins = 32\n",
    "//训练随机森林分类器，trainClassifier 返回的是 RandomForestModel 对象\n",
    "val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    " numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.07407407407407407\n",
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 428 <= 0.0)\n",
      "     If (feature 442 <= 0.0)\n",
      "      If (feature 517 <= 0.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 517 > 0.0)\n",
      "       Predict: 1.0\n",
      "     Else (feature 442 > 0.0)\n",
      "      Predict: 0.0\n",
      "    Else (feature 428 > 0.0)\n",
      "     Predict: 0.0\n",
      "  Tree 1:\n",
      "    If (feature 461 <= 0.0)\n",
      "     If (feature 323 <= 100.0)\n",
      "      Predict: 0.0\n",
      "     Else (feature 323 > 100.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 461 > 0.0)\n",
      "     Predict: 1.0\n",
      "  Tree 2:\n",
      "    If (feature 455 <= 23.0)\n",
      "     If (feature 234 <= 0.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 234 > 0.0)\n",
      "      If (feature 290 <= 36.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 290 > 36.0)\n",
      "       Predict: 0.0\n",
      "    Else (feature 455 > 23.0)\n",
      "     Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// 测试数据评价训练好的分类器并计算错误率\n",
    "val labelAndPreds = testData.map { point =>\n",
    " val prediction = model.predict(point.features)\n",
    " (point.label, prediction)\n",
    "}\n",
    "val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\n",
    "println(\"Test Error = \" + testErr)\n",
    "println(\"Learned classification forest model:\\n\" + model.toDebugString)\n",
    "\n",
    "// 将训练后的随机森林模型持久化\n",
    "model.save(sc, \"myModelPath\")\n",
    "//加载随机森林模型到内存\n",
    "val sameModel = RandomForestModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "  /**\n",
    "    * @param model 随机森林模型\n",
    "    * @param data  用于交叉验证的数据集\n",
    "    * */\n",
    "  def getMetrics(model: RandomForestModel, data: RDD[LabeledPoint]): MulticlassMetrics = {\n",
    "    //将交叉验证数据集的每个样本的特征向量交给模型预测,并和原本正确的目标特征组成一个tuple\n",
    "    val predictionsAndLables = data.map { d =>\n",
    "      (model.predict(d.features), d.label)\n",
    "    }\n",
    "    //将结果交给MulticlassMetrics,其可以以不同的方式计算分配器预测的质量\n",
    "    new MulticlassMetrics(predictionsAndLables)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " /**\n",
    "    * 在训练数据集上得到最好的参数组合\n",
    "    * @param trainData 训练数据集\n",
    "    * @param cvData 交叉验证数据集\n",
    "    * */\n",
    "  def getBestParam(trainData: RDD[LabeledPoint], cvData: RDD[LabeledPoint]): Unit = {\n",
    "    val evaluations = for (impurity <- Array(\"gini\", \"entropy\");\n",
    "                           depth <- Array(1, 20);\n",
    "                           bins <- Array(10, 300)) yield {\n",
    "      val model = RandomForest.trainClassifier(trainData, 2, Map[Int, Int](), 20, \"auto\", impurity, depth, bins)\n",
    "      val metrics = getMetrics(model, cvData)\n",
    "      ((impurity, depth, bins), metrics.precision)\n",
    "    }\n",
    "    evaluations.sortBy(_._2).reverse.foreach(println)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "//读取数据\n",
    "    //val rawData = sc.textFile(\"hdfs://192.168.1.64:8020/test/mllib/v3.csv\")\n",
    "    val rawData = MLUtils.loadLibSVMFile(sc, \"data/sample_libsvm_data.txt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,(692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50.0,48.0,238.0,252.0,252.0,252.0,237.0,54.0,227.0,253.0,252.0,239.0,233.0,252.0,57.0,6.0,10.0,60.0,224.0,252.0,253.0,252.0,202.0,84.0,252.0,253.0,122.0,163.0,252.0,252.0,252.0,253.0,252.0,252.0,96.0,189.0,253.0,167.0,51.0,238.0,253.0,253.0,190.0,114.0,253.0,228.0,47.0,79.0,255.0,168.0,48.0,238.0,252.0,252.0,179.0,12.0,75.0,121.0,21.0,253.0,243.0,50.0,38.0,165.0,253.0,233.0,208.0,84.0,253.0,252.0,165.0,7.0,178.0,252.0,240.0,71.0,19.0,28.0,253.0,252.0,195.0,57.0,252.0,252.0,63.0,253.0,252.0,195.0,198.0,253.0,190.0,255.0,253.0,196.0,76.0,246.0,252.0,112.0,253.0,252.0,148.0,85.0,252.0,230.0,25.0,7.0,135.0,253.0,186.0,12.0,85.0,252.0,223.0,7.0,131.0,252.0,225.0,71.0,85.0,252.0,145.0,48.0,165.0,252.0,173.0,86.0,253.0,225.0,114.0,238.0,253.0,162.0,85.0,252.0,249.0,146.0,48.0,29.0,85.0,178.0,225.0,253.0,223.0,167.0,56.0,85.0,252.0,252.0,252.0,229.0,215.0,252.0,252.0,252.0,196.0,130.0,28.0,199.0,252.0,252.0,253.0,252.0,252.0,233.0,145.0,25.0,128.0,252.0,253.0,252.0,141.0,37.0]))\n"
     ]
    }
   ],
   "source": [
    "val n=1\n",
    "rawData.take(n).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[268] at randomSplit at <console>:143"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   //训练集、交叉验证集和测试集，各占80%，10%和10%\n",
    "    //10%的交叉验证数据集的作用是确定在训练数据集上训练出来的模型的最好参数\n",
    "    //测试数据集的作用是评估CV数据集的最好参数\n",
    "    val Array(trainData, cvData, testData) = data.randomSplit(Array(0.8, 0.1, 0.1))\n",
    "    trainData.cache()\n",
    "    cvData.cache()\n",
    "    testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------confusionMatrix-----------------------------------------------------\n",
      "5.0  0.0  \n",
      "0.0  3.0  \n",
      "---------------------------------------------precision-------------------------------------------------\n",
      "1.0\n",
      "-----------------------------------------(precision,recall)---------------------------------------------------\n",
      "(1.0,1.0)\n",
      "(1.0,1.0)\n"
     ]
    }
   ],
   "source": [
    " //构建随机森林\n",
    "    val model = RandomForest.trainClassifier(trainData, 2, Map[Int, Int](), 20, \"auto\", \"gini\", 4, 32)\n",
    "    val metrics = getMetrics(model, cvData)\n",
    "    println(\"-----------------------------------------confusionMatrix-----------------------------------------------------\")\n",
    "    //混淆矩阵和模型精确率\n",
    "    println(metrics.confusionMatrix)\n",
    "    println(\"---------------------------------------------precision-------------------------------------------------\")\n",
    "    println(metrics.precision)\n",
    "\n",
    "    println(\"-----------------------------------------(precision,recall)---------------------------------------------------\")\n",
    "    //每个类别对应的精确率与召回率\n",
    "    (0 until 2).map(target => (metrics.precision(target), metrics.recall(target))).foreach(println)\n",
    "    \n",
    "    \n",
    "    //保存模型\n",
    "    //model.save(sc,\"hdfs://192.168.1.64:8020/tmp/RFModel\")\n",
    "\n",
    "    // 将训练后的随机森林模型持久化\n",
    "    model.save(sc, \"RandomForestModel-Save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:140: error: value split is not a member of org.apache.spark.mllib.regression.LabeledPoint\n",
       "           val values = line.split(\",\").map(_.toDouble)\n",
       "                             ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "    * 模拟对新数据进行预测1\n",
    "    */\n",
    "   val rawData = MLUtils.loadLibSVMFile(sc, \"data/sample_libsvm_data.txt\")\n",
    "\n",
    "\n",
    "  //读取模型\n",
    "  val rfModel = RandomForestModel.load(sc,\"RandomForestModel-Save\")\n",
    "  //进行预测\n",
    "  val preLabel = rfModel.predict(rawData)\n",
    "  preLabel.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
